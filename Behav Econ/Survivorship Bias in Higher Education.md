---
share: true
tags:
  - BehavioralEconomics
  - HigherEd
---
## The Invisible Failure Rate: Analyzing and Mitigating Survivorship Bias in Higher Education Administrative Decision-Making

### I. Introduction: The Epistemological Challenge in Higher Education Leadership

#### A. Defining the Scope: Survivorship Bias (SB) and the Imperative for Data Integrity in HE

Administrative decisions within Higher Education Institutions (HEIs) are predicated upon rigorous data analysis, particularly concerning institutional efficacy, student outcomes, and programmatic success. The prevailing focus, often mandated by accreditation bodies and funding requirements, is necessarily directed toward metrics of completion, retention, and positive performance. However, this focus creates a systemic vulnerability to Survivorship Bias (SB), a potent cognitive and methodological distortion that profoundly compromises administrative objectivity.

Survivorship Bias is defined as a cognitive shortcut that occurs when analysis concentrates solely on "surviving" observations or successful entities, resulting in the unintentional neglect of "failed" entities, defunct projects, or individuals who exited the process prematurely.[^1] This fundamental flaw in data collection leads to the mistaken conclusion that the successful subgroup represents the entire initial group.[^2] The consequence in the administrative sphere of higher education is systematic inflation of perceived institutional effectiveness, masking deep, structural systemic failures in areas such as student retention, policy design, and resource allocation. The failure to account for those who "left the process early" biases any subsequent forecasting or pattern recognition efforts.1

#### B. The Administrative Context: Decision-Making in an Environment of Scarcity and Strategic Ambiguity

Higher education administration operates within a complex matrix defined by public service values, political pressures, and pervasive financial constraints.3 Decision-making, fundamentally, involves a choice between several options.5 This process, vital to management work, is influenced not only by rational analysis but also by a host of subjective factors, including beliefs, culture, and various cognitive biases.5

Administrative discretion is employed by modern bureaucrats governing public affairs.3 However, when high-stakes choices—such as those related to resource allocation, enrollment management, and institutional strategic planning—are informed by incomplete data, administrative actions can inadvertently reinforce historical inequalities.4 The underlying administrative goal, therefore, is not merely to mitigate error, but to ensure that governance is objective, ethical, and evidence-based. Survivorship Bias constitutes a practical threat to this objective, as it systematically obscures the negative data required for truly ethical governance.

#### C. Structure of the Review: Mapping the Gaps in Existing Scholarship

While direct, dedicated scholarly articles explicitly linking the title "Survivorship Bias" to "Higher Education Administrative Decision-Making" may be sparse, a robust body of research confirms SB's role as a severe methodological risk in relevant domains, including student retention studies 6 and organizational management.7 Furthermore, the susceptibility of HEI strategic planning to related cognitive biases is well-established.5

This analysis transitions from general management and behavioral economics theory to establish the specific applicability and impact of SB across core administrative functions. A crucial preliminary determination is that SB is not merely a technical error but rather a systemic policy failure. The failure to collect or analyze data on dropouts or terminated initiatives 1 is frequently driven by an administrative focus on success metrics required for external validation. This prioritization effectively transforms the methodological error into a systemic policy choice—an ethical lapse in accounting for marginalized or unsuccessful stakeholders. Consequently, mitigating SB must be reframed within the HE context as an effort to achieve institutional fairness and equity, moving beyond basic efficiency concerns.

### **II. Theoretical Foundations of Survivorship Bias**

#### **A. Historical Origins: From Military Strategy to Behavioral Economics**

The recognition of survivorship bias as a formal analytical phenomenon traces its origins to World War II. During this period, statistician Abraham Wald examined Allied aircraft returning from missions. He observed that the areas exhibiting the *least* amount of bullet damage were the ones needing reinforced armor, because the aircraft that suffered hits in the most critical areas (e.g., engines, cockpits) did not "survive" the flight back.7 The historical application confirms the core definition: SB arises when the sample of observed data is biased because non-surviving observations are systematically excluded, leading to profoundly skewed conclusions about the characteristics necessary for success.2

In the corporate and professional world, SB influences project professionals who tend to look exclusively at projects that completed successfully to identify desirable patterns for repeatability in process and practice.7 They seek out patterns modeled by successful leaders, failing to study the common factors present in failed projects.7

#### **B. Cognitive Architecture: SB as a Heuristic Failure**

Survivorship bias functions as a potent cognitive shortcut rooted in the availability heuristic and confirmation bias.2 Successful examples are far more readily available, receive greater public attention, and are often more emotionally compelling than stories of failure. This leads individuals, including senior administrators, to perceive a high "weight of evidence" when reviewing success narratives, even when ambiguity regarding the underlying causal factors remains.10 When the perceived weight of evidence is high, the desire to close the knowledge gap concerning possible failures decreases, making the administrator less likely to seek out missing information about failed outcomes.10

Furthermore, the prevalence of SB drives a pervasive policy phenomenon referred to as the "Magic Formula" fetish, particularly evident in the corporate world where "unicorn start-ups" are celebrated as archetypes of success.2 The narratives of successful founders like Steve Jobs or Mark Zuckerberg lead observers to conclude, erroneously, that success is achievable by following a simple, replicable formula (e.g., "drop out and focus on the big idea").2 This overlooks the thousands of individuals who followed similar paths and failed, whose stories are simply not widely shared.2

When transferred to higher education administration, this syndrome translates into the uncritical replication of "best practice" policies or "successful leader" models from peer institutions.7 This approach neglects the complex, often non-replicable contextual failures, resource advantages, or elements of pure chance that may have preceded the success. Institutional policy formulation is thus often dangerously simplified by the survivor narrative.

#### **C. The Spectrum of Selection Bias: Differentiating SB from other forms of Data Exclusion**

While often grouped under the broader category of selection bias, SB possesses a distinct character that affects forecasting and strategic planning. It is specifically characterized by the exclusion of entities that did not endure to the end of a given period.1

The psychological comfort offered by simplified success narratives reinforces administrative tendencies, particularly those driven by the desire for control. The illusion of control and the specific psychological "need for control" strongly influence managerial decisions.11 SB feeds this illusion by supplying neat, conclusive narratives of successful outcomes, which suggest that the administrative or strategic process is inherently controllable and predictable. By diverting attention away from complex, chaotic, or unpredictable failure vectors, SB enables managers to believe the process is more manageable than it actually is, potentially leading to overconfidence or rigidity in strategy.11 Effective mitigation of SB therefore necessitates addressing this organizational desire for simplicity and control, making it an issue of institutional culture as well as data rigor.

### **III. Survivorship Bias in Organizational Management and Project Success (Extrapolating to HE)**

#### **A. The Failure of Performance Benchmarking: Financial and Corporate Parallels**

The impact of survivorship bias is perhaps most rigorously studied in financial markets. Studies evaluating the effects of SB on portfolio returns found that using only data series that survived till the end of the period skewed results.12 When researchers corrected for the missing (failed) firms, earlier claims regarding positive effects (such as the P/E effect on returns) were rejected.12 Similarly, analyses of investment performance findings must confirm they are survivorship-bias free and robust after controlling for fund characteristics.13

The extrapolation of this phenomenon to HEIs reveals institutional financial survivorship bias. Universities frequently engage in peer benchmarking based on visible metrics like endowment growth, high research rankings, or successful program scaling. If the selection of peer institutions focuses only on those that appear highly solvent and successful, the HEI falls victim to institutional SB. This leads to the adoption of operational or financial models without understanding the systemic financial or operational vulnerabilities (the hidden failures) that may have driven other, similar peer institutions into distress or failure. This fundamentally distorts the risk assessment associated with adopting such "successful" strategies.

#### **B. Impact on Process Design and Project Management Repeatability**

In the context of organizational management, the common constraint and practices of project management are heavily focused on Scope, Schedule, and Cost.7 SB reinforces a tendency for project professionals to look exclusively at completed, successful projects to identify patterns for repeatability.7 They forget the critical lessons provided by projects that failed, were quietly terminated, or bled resources before being shelved.7

Applied to higher education, this manifests when an HEI launches major multi-year administrative projects—such as enterprise resource planning (ERP) system upgrades, massive curriculum overhauls, or new online learning initiatives. The evaluation process systematically favors the completed, functional phases. Failed pilot programs, initiatives terminated due to political resistance, or projects that experienced cost overruns before eventual cancellation are ignored in the official post-mortem. This leads to the dangerous replication of flawed project management constraints and perpetuates an inaccurate assessment of institutional capacity for successful delivery. If failure is rendered invisible, the institution cannot improve its resilience or project governance structure.

#### **C. Human Capital Forecasting and the Attrition Problem**

Survivorship bias also distorts human capital analysis, particularly in recruitment forecasting, where analysis focuses only on those employees who "survived" the period, neglecting those who left early.1 Higher education sectors are characterized by significant employee attrition rates, which compels management to develop long-term career plans to retain staff and faculty.1

When analyzing faculty and staff career trajectories, SB arises by focusing exclusively on tenured faculty, highly successful senior administrators, or those who achieve rapid promotion (often analyzed through frameworks like Leader-Member Exchange theory, LMX).14 This analysis may mistakenly attribute success to superficial leadership traits—such as humility—when the success may, in reality, be due to systemic advantages, biased evaluation processes, or sheer luck (the survivorship bias effect).14 This approach neglects to systematically study the systemic factors causing attrition among adjunct, early-career, or marginalized employees. The result is an HR strategy that replicates retention strategies for those already favored by the system (the survivors) while failing to diagnose the structural issues driving the departure of vulnerable or dissatisfied cohorts.

The systematic effect of survivorship bias in various organizational domains, extrapolated to higher education, demonstrates its pervasive influence on policy and strategy.

Table 1

Typology of Survivorship Bias: From General Theory to Administrative Application

| Domain of Study | What 'Survives' (Observed) | What 'Fails' (Ignored) | Consequence for HEI Decision-Making |
| :---- | :---- | :---- | :---- |
| Financial Markets (General) | Companies listed at the end of a period (e.g., successful endowments).12 | Delisted or bankrupt entities; financially distressed peers. | Inflation of perceived returns on investment or adoption of financially unsustainable peer models. |
| Project Management | Completed, successful administrative projects or policies.7 | Projects halted, failed, or terminated due to organizational friction. | Replication of flawed project practices; inadequate risk mitigation strategies. |
| Higher Education (Students) | Retained students (currently enrolled and responsive).6 | Students who dropped out or whose data trace is lost.1 | Misinterpretation of retention strategies; policy creation that serves existing successful students (survivors). |
| Higher Education (Personnel/Programs) | Leaders who achieved professional success; highly-ranked programs.14 | Leaders/faculty who left prematurely; discontinued or high-attrition programs. | Mistaken attribution of success to individual traits over systemic factors; continued funding of potentially underperforming areas. |

This systematic bias creates institutional inertia and inhibits innovation. By benchmarking against survivor peers and replicating internal "successful" processes, institutions systematically filter out high-risk, potentially transformative strategies that failed early. This results in the institutionalization of risk aversion, leading to the systematic over-allocation of resources to perceived "winners" (e.g., highly visible programs or successful departments), while starving necessary exploratory or experimental initiatives.

### **IV. The Influence of Cognitive Biases on Higher Education Governance**

#### **A. Decision-Making Frameworks in Public Administration and HE**

Administrative decision-making is positioned at the nexus of public service values and pragmatic governance.3 Public administrators utilize discretion as they govern public affairs within a political and ethical context.3 As the defining activity of management, the quality of decision-making is paramount, yet it is inherently vulnerable to numerous cognitive factors.5

#### **B. Survey of Identified Cognitive Biases in HE Strategic Planning**

Scholarly research explicitly confirms that cognitive biases significantly and detrimentally impact university strategic planning.8 Studies have identified several key biases that plague institutional decisional processes, including Confirmation Bias, Overconfidence Bias, Anchoring Bias, and Loss Aversion.8 Structured, evidence-based decision-making frameworks are necessary to cultivate a more rational approach and reduce the detrimental impact of these cognitive shortcuts.8

Survivorship Bias functions as a highly relevant derivative in this spectrum, acting as a specialized form of Confirmation Bias combined with the Availability Heuristic.2 Administrators preferentially seek and confirm information that aligns with observed institutional successes, because success data is the most readily available and publicized.

#### **C. The Role of Institutional Culture: Authority, Leadership, and the Perception of Success**

The cultural context of higher education governance influences its susceptibility to biases. Institutional environments that promote liberal education mitigate against authoritarian tendencies by exposing individuals to diverse contexts, histories, ideas, and ways of life.3 By its very definition, survivorship bias filters out failure and non-dominant perspectives—the invisible subgroup—and therefore fundamentally works against the core pedagogical mission of fostering intellectual complexity and critical inquiry.2 Addressing administrative SB becomes, therefore, an institutional ethical responsibility aligned with the HEI’s broader mission of promoting diversity of thought and methodology.15

Conversely, an administrative culture that excessively promotes and rewards high performance without formal mechanisms for discussing or learning from failure will naturally perpetuate SB. Managers often succumb to the illusion of control, believing organizational change processes are more controllable and predictable than they truly are.11 When institutional change efforts are evaluated exclusively based on successful outcomes or completed segments, SB reinforces this dangerous illusion, making managers resistant to external criticism or necessary systemic adjustments.

To counteract this, the organization must prioritize shared leadership and respectful communication, where leaders (such as department chairs) serve as sources of support rather than strict managers of faculty.14 Even in shared governance structures, however, SB can persist in the data inputs unless mechanisms are established to formally challenge the administrative sense of control derived from survivor data.

### **V. Survivorship Bias in Core Higher Education Administrative Functions**

#### **A. Student Success, Retention, and Attrition**

The most direct scholarly evidence of survivorship bias in higher education policy concerns student retention and attrition research. SB is a recognized methodological flaw when studies are limited solely to currently enrolled students.6 In one research thesis, the author noted the research suffered from survivorship bias because, despite attempts to contact them via calls and emails, drop-out students could not be reached, limiting the sample to those who were retained.6

This exclusion severely biases findings regarding the effectiveness of retention policies and forecasting efforts. Policy analysis is conducted based on the experiences of the "survivors"—those who successfully navigated institutional barriers, financial aid policies, and complex registration systems.1 Consequently, institutional research (IR) misreads the needs of students at high risk of dropping out, as the policies created inherently favor the profile of the retained student. This also applies to program evaluation: judging the efficacy of a course or curriculum based only on successful completion rates fails if the analysis ignores high withdrawal rates or high failure rates followed by success only after subsequent remedial intervention.

#### **B. Predictive Analytics (PA) and Data Bias Mitigation**

The rise of predictive analytics (PA) in higher education governance—used for enrollment management, resource allocation, and student success strategies—makes the latent issue of SB critically important.4 Predictive models are trained on historical data sets.16 If this historical data reflects only historically successful cohorts (the survivors), the models implicitly learn to identify success based on the characteristics of those cohorts. This results in the automation and perpetuation of historical exclusion, potentially leading to unfair outcomes, such as models tending to predict success more frequently for White and Asian students compared to Black and Hispanic students in certain contexts (a failure of statistical parity).17

While HEIs are rightly taking steps to address bias using advanced tools like Aequitas, an open-source bias audit toolkit, and by employing fairness metrics such as the Brier Score or Balance for Negative/Positive Class 18, these measures often overlook selection bias inherent in the data collection itself. SB represents a fundamental *pre-modeling* data quality issue: the administrative system has learned only from those who were retained long enough to generate adequate longitudinal data points. Therefore, regardless of how robust the *post-modeling* bias mitigation techniques are, if the training data is survivor-biased, the model will struggle to generalize accurately to non-survivor populations.17 This process creates an "illusion of fairness" where technically compliant algorithms perpetuate the invisible bias embedded in the source data.

#### **C. Resource Allocation and Program Evaluation**

Survivorship bias critically distorts resource allocation decisions. Administrators use internal and external data to inform resource allocation and strategic planning.4 Decisions regarding fiscal resources—covering faculty, administrative support, and general operating expenses—for new or ongoing programs are often justified by linking them to demonstrable success metrics or institutional goals.4

SB occurs when program evaluation focuses exclusively on the demonstrated long-term success of established, ongoing initiatives (the survivors), neglecting the lessons or opportunity costs associated with terminated programs, pilot projects, or underperforming departments that were quietly defunded. For instance, while the success of an initiative like the Social Justice Alliance can be measured 4, institutional analysis must also systematically track the long-term impacts of unsuccessful or quickly dismantled departmental social initiatives. Without this failure data, resource allocation will systematically flow toward the perceived "winners," leading to a magnification of the "rising tide" effect (benefiting the majority or existing successful groups) while fundamental systemic inequalities remain unaddressed.17

The administrative functions listed below illustrate the critical linkage between data integrity and equitable policy outcomes, underscoring why SB must be formally addressed in HE governance.

Table 2

Administrative Functions Susceptible to Survivorship Bias in HEIs

| Administrative Function | Primary Data Deficiency Introduced by SB | Impact on Administrative Policy & Equity |
| :---- | :---- | :---- |
| **Predictive Analytics (PA) for Student Success** | Models trained solely on historical success data.16 | Automating historical bias; models struggle to identify potential success in non-traditional or high-risk cohorts (false negatives for success). |
| **Recruitment and Enrollment Management** | Failure to track reasons why qualified admitted students enroll elsewhere or defer.1 | Over-reliance on optimizing communications for those who accepted (the survivors), neglecting structural barriers for non-enrollers. |
| **Curriculum & Program Review** | Evaluation based only on high-performing alumni or sustained, positive course outcomes. | Failure to diagnose structural issues in high-demand, high-failure rate foundational courses, reinforcing pedagogical conservatism.15 |
| **Institutional Assessment/Accreditation** | Reporting exclusively on sustained, positive outcomes (e.g., successful accreditation renewals). | Inflated institutional self-perception; failure to fund necessary institutional risk-taking or deep systemic reform.8 |
| **Ethical AI Governance** | Focus on *post-modeling* bias mitigation (fairness metrics) rather than *data collection* bias (selection error).18 | Creation of an 'illusion of fairness,' where technically compliant models perpetuate the invisible bias embedded in the survivor dataset. |

The root of the problem is often structural: students who drop out or programs that fail leave a minimal, incomplete, or messy data trace.6 Administrative systems are primarily optimized for clean, longitudinal data collection from active participants. This means SB is structurally embedded in the data infrastructure itself, creating a feedback loop where failure is rendered invisible because the mechanism to track it is not prioritized administratively. Furthermore, this bias extends to the academic environment itself, leading to **Pedagogical Survivorship Bias**, where the administration reinforces conventional teaching methods by evaluating success based only on established curriculum outcomes and published research that successfully passed peer review, potentially excluding or devaluing innovative, high-risk, or non-traditional pedagogical approaches.15

### **VI. Mitigating Survivorship Bias in Institutional Research and Policy**

#### **A. Data Integrity and Collection: Designing Datasets to Capture Failure**

Mitigation must begin by confronting the structural data deficiencies that facilitate SB. Institutional Research (IR) must move beyond relying on existing administrative data sources and establish proactive, dedicated protocols for collecting data from non-enrolled or departed cohorts.1

Systemic inclusion of non-survivor data is paramount. This requires institutional investment in "failure tracking infrastructure" and mandated data standards for collecting and preserving data on attrition, project termination, and policy failure. Collecting this data from dropouts, for instance, may require independent third-party confidential exit surveys to overcome response reluctance.6

Bias auditing must be expanded to rigorously test for selection error (SB). While current toolkits focus on demographic parity (e.g., Aequitas 18), the evaluation must compare the demographic and socioeconomic profile of the training dataset used for predictive modeling against the profile of the *entire* target population, including non-survivors. Furthermore, the calibration of predictive models, measured by metrics like the Brier Score 19, is essential. Poor calibration often signals that the model is learning from a non-representative, survivor-biased training set, necessitating data augmentation or re-weighting techniques to incorporate the impact of failure scenarios.

#### **B. Administrative Debiasing Techniques and Policy Reform**

Overcoming SB requires formalized administrative debiasing techniques that challenge the cultural preference for success narratives.

1. **Integrating Failure Analysis:** Formal processes for conducting "lessons learned" from unsuccessful policies or projects are necessary, mirroring post-mortems used in engineering or military contexts.7 This requires transparent documentation and analysis of failure causes, rather than simply moving on to the next successful initiative.  
2. **The "Pre-Mortem" Technique:** Implementing structured decision-making protocols, where administrators are explicitly instructed to imagine that the current initiative has already failed catastrophically a year in the future, and then work backward to identify the causes of that failure.10 This technique forces the consideration of non-survivor pathways and significantly raises the perceived weight of evidence regarding potential failure, thus countering the cognitive comfort provided by the illusion of control.  
3. **Promoting Cognitive Humility:** Fostering an administrative culture that recognizes the limits of intuition and success-driven narratives is essential.11 Structured, evidence-based frameworks are necessary to reduce the detrimental impact of cognitive biases across strategic planning.8

Crucially, the conflict between bias mitigation and administrative transparency must be addressed. While ethical AI governance demands transparency and interpretability of algorithms 18, some debiasing techniques focused purely on algorithmic adjustment may obscure the underlying selection bias, leading to an illusion of fairness.17 Mitigation efforts must therefore focus on **data source integrity** (de-biasing the input) rather than solely **algorithmic adjustment** (de-biasing the output) to maintain ethical responsibility and stakeholder trust.16

Finally, the financial justification for capturing failure data must be articulated. Tracking dropouts, funding research into failed programs, and building new IT infrastructure for failure analysis represents a direct cost.4 SB naturally results from the administrative inclination to optimize resource use based on visible success. However, operating based on flawed, survivor-biased metrics leads to the long-term cost of repeating failed projects and perpetuating inefficient resource allocation. The investment in comprehensive failure analysis must be strategically framed as a cost-avoidance measure that improves long-term institutional resilience.

### **VII. Conclusion and Future Research Agenda**

#### **A. Synthesis of Findings: Scholarly Recognition of SB in HE Administration**

Scholarly research confirms that while direct, dedicated empirical studies on survivorship bias in higher education administrative decision-making are not widely indexed under this explicit terminology, the impact of SB is both recognized and systemic across HEI operations.

Direct evidence confirms SB’s methodological impact in key administrative data domains, notably in student retention research where the exclusion of non-survivor cohorts biases policy conclusions.1 Strong indirect evidence synthesizes SB with broader cognitive bias literature affecting strategic planning, financial benchmarking, and predictive modeling.8

The analysis concludes that Survivorship Bias is a recognized, potent, and often unaddressed source of systemic administrative policy error in higher education. It fundamentally distorts institutional self-perception, undermines efforts toward equity in predictive analytics, and inhibits necessary organizational learning from failure.

#### **B. Recommendations for Administrative Practice and Policy Reform**

Based on the evidence reviewed, the following actionable recommendations are proposed for higher education administrators and institutional researchers:

1. **Mandate Formal Failure Audits:** Implement a structured process, such as the pre-mortem technique, requiring formal failure analysis for all major institutional projects, policy changes, and technological implementations. These audits must document and disseminate lessons learned from terminated or underperforming initiatives.7  
2. **Institutional Research Data Standards:** Require Institutional Research to utilize non-survivor data metrics when forecasting student success, enrollment, and human capital needs. This necessitates establishing protocols for ethically collecting and rigorously analyzing data from departed students, faculty, and staff, explicitly mitigating selection bias.1  
3. **Ethical AI Governance Priority:** Establish ethical governance structures for AI and predictive analytics that prioritize the identification and remediation of selection bias in training data as the primary ethical challenge, ahead of post-modeling demographic adjustments. Models must be calibrated using metrics that account for the non-representative nature of survivor data.16

#### **C. Proposed Research Trajectories: Empirical Studies on SB and HEI Resource Management**

To further solidify the scholarly understanding and mitigation strategies, the following research trajectories are recommended:

1. **Resource Allocation Impact:** Longitudinal comparative studies analyzing resource allocation decisions made by HEIs that utilize structured failure analysis (failure audits) versus those relying solely on success-based performance metrics. The study should quantify the long-term efficiency and innovative capacity differences between the two groups.  
2. **Predictive Model Validation:** Quantitative studies comparing the predictive performance, accuracy, and fairness metrics of student success models trained exclusively on survivor data against those models augmented with comprehensive, verified dropout data, focusing on model calibration and the reduction of false negative predictions for at-risk cohorts.  
3. **Cultural and Leadership Factors:** Empirical investigations into the correlation between HEI governance structures (e.g., shared leadership versus top-down authority) and the institutional openness to incorporating failure data, potentially linking administrative humility to the effective mitigation of survivorship bias in policy formulation.11

##### **Works cited**

1.
2. Survivorship bias \- The Decision Lab, accessed November 7, 2025, [https://thedecisionlab.com/biases/survivorship-bias](https://thedecisionlab.com/biases/survivorship-bias)  
3. Empowering Public Administrators \[1 ed.\] 1032651822, 9781032651828 \- DOKUMEN.PUB, accessed November 7, 2025, [https://dokumen.pub/empowering-public-administrators-1nbsped-1032651822-9781032651828.html](https://dokumen.pub/empowering-public-administrators-1nbsped-1032651822-9781032651828.html)  
4. Education Policy and Student Life \- May 11, 2023 \- University System of Maryland, accessed November 7, 2025, [https://www.usmd.edu/regents/agendas/20230511-EPSL-PublicSession.pdf](https://www.usmd.edu/regents/agendas/20230511-EPSL-PublicSession.pdf)  
5. Internal processes in decision-making (mental, emotional, cultural ..., accessed November 7, 2025, [https://www.emerald.com/insight/content/doi/10.1108/PRR-10-2020-0037/full/html](https://www.emerald.com/insight/content/doi/10.1108/PRR-10-2020-0037/full/html)  
6. A Case Study Examining Retention Strategy in a Private Lebanese ..., accessed November 7, 2025, [https://eprints.staffs.ac.uk/3980/1/YassineDM\_DBA%20thesis.pdf](https://eprints.staffs.ac.uk/3980/1/YassineDM_DBA%20thesis.pdf)  
7. "Human Centered Projects and Survivorship Bias" by Tom Wise, accessed November 7, 2025, [https://digitalcommons.harrisburgu.edu/beyond\_project\_horizon/vol1/iss1/4/](https://digitalcommons.harrisburgu.edu/beyond_project_horizon/vol1/iss1/4/)  
8. The Impact of Cognitive Biases on Strategic Decision-Making | \`, accessed November 7, 2025, [https://assajournal.com/index.php/36/article/view/413](https://assajournal.com/index.php/36/article/view/413)  
9. Law and order effects: on cognitive dissonance and belief perseverance \- PubMed Central, accessed November 7, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC9186347/](https://pmc.ncbi.nlm.nih.gov/articles/PMC9186347/)  
10. Human Centered Projects and Survivorship Bias The Perceived Success of Project Outcomes?, accessed November 7, 2025, [https://digitalcommons.harrisburgu.edu/context/beyond\_project\_horizon/article/1008/viewcontent/Human\_Centered\_Projects\_\_\_Survivorship\_Bias.pdf](https://digitalcommons.harrisburgu.edu/context/beyond_project_horizon/article/1008/viewcontent/Human_Centered_Projects___Survivorship_Bias.pdf)  
11. Cognitive Biases in Strategic Decision-Making \- MDPI, accessed November 7, 2025, [https://www.mdpi.com/2076-3387/15/6/227](https://www.mdpi.com/2076-3387/15/6/227)  
12. Understanding Survivorship Bias: Implications for Research and Decision-Making \- JETIR.org, accessed November 7, 2025, [https://www.jetir.org/papers/JETIR2406276.pdf](https://www.jetir.org/papers/JETIR2406276.pdf)  
13. e-proceedings \- IRIS, accessed November 7, 2025, [https://iris.unive.it/retrieve/e4239ddb-7975-7180-e053-3705fe0a3322/WFC\_eproceedings\_Complete\_IPO.pdf](https://iris.unive.it/retrieve/e4239ddb-7975-7180-e053-3705fe0a3322/WFC_eproceedings_Complete_IPO.pdf)  
14. leader-member exchange from the perspective of people with disabilities \- ResearchGate, accessed November 7, 2025, [https://www.researchgate.net/publication/354951597\_LEADER-MEMBER\_EXCHANGE\_FROM\_THE\_PERSPECTIVE\_OF\_PEOPLE\_WITH\_DISABILITIES](https://www.researchgate.net/publication/354951597_LEADER-MEMBER_EXCHANGE_FROM_THE_PERSPECTIVE_OF_PEOPLE_WITH_DISABILITIES)  
15. Understanding the Lived Experiences of the members of the Society for the Advancement of Biology Education Research through Collins' Matrix of Domination Framework | CBE—Life Sciences Education, accessed November 7, 2025, [https://www.lifescied.org/doi/10.1187/cbe.24-02-0074](https://www.lifescied.org/doi/10.1187/cbe.24-02-0074)  
16. AI-Enhanced Decision Support Systems for Strategic Higher Education Management, accessed November 7, 2025, [https://hrmars.com/papers\_submitted/26819/ai-enhanced-decision-support-systems-for-strategic-higher-education-management-a-framework-for-improving-decision-making-efficiency-and-stakeholder-trust.pdf](https://hrmars.com/papers_submitted/26819/ai-enhanced-decision-support-systems-for-strategic-higher-education-management-a-framework-for-improving-decision-making-efficiency-and-stakeholder-trust.pdf)  
17. Exploring Bias Mitigation Techniques for Predictive ... \- UIC Indigo, accessed November 7, 2025, [https://indigo.uic.edu/articles/thesis/Exploring\_Bias\_Mitigation\_Techniques\_for\_Predictive\_Analytics\_in\_College\_Student\_Success/29048798/1/files/54497096.pdf](https://indigo.uic.edu/articles/thesis/Exploring_Bias_Mitigation_Techniques_for_Predictive_Analytics_in_College_Student_Success/29048798/1/files/54497096.pdf)  
18. Harnessing the Era of Artificial Intelligence in Higher Education, accessed November 7, 2025, [https://ysu.edu/sites/default/files/institute\_teaching\_learning/ai-for-educators/ai\_in\_higher\_education.pdf](https://ysu.edu/sites/default/files/institute_teaching_learning/ai-for-educators/ai_in_higher_education.pdf)  
19. Interactive Mitigation of Biases in Machine Learning Models for Undergraduate Student Admissions \- MDPI, accessed November 7, 2025, [https://www.mdpi.com/2673-2688/6/7/152](https://www.mdpi.com/2673-2688/6/7/152)  
20. Cognitive Biases and Decision Making: A Literature Review and Discussion of Implications for the US Army, accessed November 7, 2025, [https://cgsc.contentdm.oclc.org/digital/api/collection/p16040coll2/id/19/download](https://cgsc.contentdm.oclc.org/digital/api/collection/p16040coll2/id/19/download)

#BehavioralEconomics #HigherEd

[^1]: Principles Of Management 9789332530812, 9789332544680 ..., accessed November 7, 2025, [https://ebin.pub/principles-of-management-9789332530812-9789332544680-5145175175-9332530815.html](https://ebin.pub/principles-of-management-9789332530812-9789332544680-5145175175-9332530815.html)  

[^2]: Bah bah black sheep
